{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb4d5a2-cb98-4f44-864f-42e804fef5e9",
   "metadata": {},
   "source": [
    "### Approach to the Solution\n",
    "\n",
    "1. **Data Extraction:**\n",
    "   - **Reading Input:** The script begins by reading an Excel file (Input.xlsx) using pandas to obtain URLs and their corresponding IDs.\n",
    "   - **Fetching Article Text:** For each URL, it uses requests and BeautifulSoup to extract the article text from the webpage. The extracted text is saved into separate text files named after their URL_IDs in a directory (articles/).\n",
    "\n",
    "2. **Textual Analysis:**\n",
    "   - **Loading Dependencies:** The script uses several libraries and tools:\n",
    "     - pandas: For data handling and manipulation.\n",
    "     - nltk: For tokenization (word_tokenize, sent_tokenize), stopword removal, and other linguistic analyses.\n",
    "     - textstat: For readability metrics such as syllable count and Gunning Fog index.\n",
    "     - TextBlob: For sentiment analysis (polarity and subjectivity).\n",
    "   - **Calculating Metrics:** Various functions are defined to compute different metrics such as average sentence length, percentage of complex words, fog index, and more. These functions utilize NLTK and textstat functionalities to analyze the text extracted from each article file.\n",
    "   - **Iterating through Data:** It iterates through each row of the input DataFrame, reads the corresponding article text from the saved files, calculates the required metrics, and stores the results in lists within output_data.\n",
    "\n",
    "3. **Output Generation:**\n",
    "   - **Creating Output DataFrame:** After computing all metrics for all articles, it creates a pandas DataFrame (output_df) from output_data.\n",
    "   - **Saving to Excel:** Finally, the script saves this DataFrame into an Excel file named Output Data Structure.xlsx, ensuring that the structure and content match the requirements specified in Output Data Structure.xlsx.\n",
    "\n",
    "### Running the .py File to Generate Output\n",
    "\n",
    "To run the Python script (analysis.ipynb) and generate the output (Output Data Structure.xlsx), follow these steps:\n",
    "\n",
    "1. **Setup Dependencies:**\n",
    "   - Ensure you have installed the required libraries:\n",
    "     \n",
    "     !pip install pandas requests beautifulsoup4 nltk textstat textblob\n",
    "     \n",
    "\n",
    "2. **Prepare Input Data:**\n",
    "   - Place your Input.xlsx file in the same directory as your Python script (analysis.ipynb). Ensure it contains the required URLs and URL_IDs.\n",
    "\n",
    "3. **Run the Script:**\n",
    "   - Open a jupiter notebook .\n",
    "   - Navigate to the directory containing analysis.ipynb and Input.xlsx.\n",
    "   - Run code :\n",
    "     \n",
    "     \n",
    "   - This will start the script, which will fetch articles from the URLs, perform textual analysis, and save the results to 'Output Data Structure.xlsx'.\n",
    "\n",
    "### Dependencies Required\n",
    "\n",
    "Ensure that you have the following dependencies installed:\n",
    "\n",
    "- pandas: For data manipulation and Excel file handling.\n",
    "- requests: For making HTTP requests to fetch web pages.\n",
    "- beautifulsoup4: For parsing HTML and extracting content from web pages.\n",
    "- nltk: For natural language processing tasks such as tokenization and stopword removal.\n",
    "- textstat: For computing readability metrics like syllable count and Gunning Fog index.\n",
    "- textblob: For sentiment analysis.\n",
    "\n",
    "You can install these dependencies using pip:\n",
    "\n",
    "bash\n",
    "pip install pandas requests beautifulsoup4 nltk textstat textblob\n",
    "\n",
    "\n",
    "By following these steps and ensuring the correct setup of dependencies, you should be able to successfully run the script and generate the required textual analysis output in Excel format. \n",
    "If you encounter any issues or need further assistance, feel free to ask!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c27cf8e-adb3-4dfa-a3c8-b031220f6f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas requests beautifulsoup4 nltk textstat textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17086f5e-99ea-482d-a005-1d723e09fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SIDDHARTH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SIDDHARTH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SIDDHARTH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load positive and negative word lists\n",
    "positive_words = set(open('positive-words.txt').read().split())\n",
    "negative_words = set(open('negative-words.txt').read().split())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the input Excel file\n",
    "input_df = pd.read_excel('C:\\\\Users\\\\SIDDHARTH\\\\Downloads\\\\Input.xlsx')\n",
    "\n",
    "# Create a directory to save the extracted articles\n",
    "os.makedirs('articles', exist_ok=True)\n",
    "\n",
    "# Log file to keep track of errors\n",
    "error_log = open('error_log.txt', 'w', encoding='utf-8')\n",
    "\n",
    "# Function to count syllables\n",
    "d = cmudict.dict()\n",
    "def syllable_count(word):\n",
    "    return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0] if word.lower() in d else 0\n",
    "\n",
    "# Function to analyze text\n",
    "def analyze_text(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Clean words by removing stop words and punctuation\n",
    "    cleaned_words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    \n",
    "    # Calculate word counts and syllables\n",
    "    word_count = len(cleaned_words)\n",
    "    syllable_per_word = sum(syllable_count(word) for word in cleaned_words) / word_count if word_count else 0\n",
    "    avg_word_length = sum(len(word) for word in cleaned_words) / word_count if word_count else 0\n",
    "    \n",
    "    # Calculate positive, negative scores\n",
    "    positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "    \n",
    "    # Calculate polarity and subjectivity\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
    "    \n",
    "    # Calculate sentence length and complex words\n",
    "    avg_sentence_length = word_count / len(sentences) if sentences else 0\n",
    "    complex_words = [word for word in cleaned_words if syllable_count(word) > 2]\n",
    "    complex_word_count = len(complex_words)\n",
    "    percentage_of_complex_words = complex_word_count / word_count if word_count else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "    \n",
    "    # Calculate average number of words per sentence\n",
    "    avg_words_per_sentence = word_count / len(sentences) if sentences else 0\n",
    "    \n",
    "    # Count personal pronouns\n",
    "    personal_pronouns = sum(1 for word in cleaned_words if word.lower() in ['i', 'we', 'my', 'ours', 'us'])\n",
    "    \n",
    "    return [\n",
    "        positive_score,\n",
    "        negative_score,\n",
    "        polarity_score,\n",
    "        subjectivity_score,\n",
    "        avg_sentence_length,\n",
    "        percentage_of_complex_words,\n",
    "        fog_index,\n",
    "        avg_words_per_sentence,\n",
    "        complex_word_count,\n",
    "        word_count,\n",
    "        syllable_per_word,\n",
    "        personal_pronouns,\n",
    "        avg_word_length\n",
    "    ]\n",
    "\n",
    "# Analyze each article and compile the results\n",
    "results = []\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    file_path = f'articles/{url_id}.txt'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            analysis_results = analyze_text(text)\n",
    "            results.append([url_id] + analysis_results)\n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # This will raise an HTTPError for bad responses\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract the article title and text (adjust the selector based on the website structure)\n",
    "            title = soup.find('h1').get_text(strip=True)\n",
    "            article_body = soup.find('div', class_='td-post-content')\n",
    "            paragraphs = article_body.find_all('p')\n",
    "            article_text = ' '.join([para.get_text(strip=True) for para in paragraphs])\n",
    "            \n",
    "            # Save the extracted text to a file named after the URL_ID\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(title + '\\n' + article_text)\n",
    "            \n",
    "            # Perform analysis on the extracted text\n",
    "            analysis_results = analyze_text(title + ' ' + article_text)\n",
    "            results.append([url_id] + analysis_results)\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            error_log.write(f\"Failed to fetch article for URL_ID {url_id}: {e}\\n\")\n",
    "        except Exception as e:\n",
    "            error_log.write(f\"Failed to extract article for URL_ID {url_id}: {e}\\n\")\n",
    "\n",
    "error_log.close()\n",
    "\n",
    "# Convert results to DataFrame\n",
    "columns = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', \n",
    "           'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', \n",
    "           'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Merge with input data\n",
    "final_df = pd.merge(input_df, results_df, on='URL_ID')\n",
    "\n",
    "# Save the final results to Excel\n",
    "final_df.to_excel('Output Data Structure.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65dae9-b8b2-4c19-a676-78309d43a5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
